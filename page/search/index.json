[{"content":"nachos file system if we want to show a project, we will \u0026hellip;\n","date":"2024-02-05T19:49:28+08:00","permalink":"https://leo00932302.github.io/post/nachos/","title":"file system"},{"content":"作業系統簡介 此網站為作業系統的筆記，涵蓋每個單元，包含附錄。\n第一章 第二章 第三章 ","date":"2024-01-31T00:08:50+08:00","permalink":"https://leo00932302.github.io/post/os/","title":"作業系統"},{"content":"大一上學期(個體經濟學期中考) 大一上學期(個體經濟學期末考) 大一下學期(總體經濟學期中考) 大一下學期(總體經濟學期末考) ","date":"2024-01-31T00:08:50+08:00","permalink":"https://leo00932302.github.io/post/%E8%80%83%E5%8F%A4%E9%A1%8C/%E7%B6%93%E6%BF%9F%E5%AD%B8/","title":"經濟學考古題"},{"content":"binary search tree #include \u0026lt;stdio.h\u0026gt; int binsearch(int yarr[10], int element) { int mid = sizeof(yarr) % 2; int left = (int)yarr / 2 - mid; int right = yarr - left; extern int i; if(element == mid) printf(\u0026#34;%d\u0026#34;, \u0026amp;mid); if(element != yarr[right]) { for(i = 0; i \u0026lt; left; i++) { if(element == i) { //Does this so that it doesn\u0026#39;t print multiple times printf(\u0026#34;%d\u0026#34;, i); } } } }; int main(int argc, int argv[]) { binsearch(argv[0], argv[1]); } bst implement // Implementation of Binary Search Tree #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; struct BST { int data; struct BST* left; struct BST* right; }; struct BST *CreateNode() { struct BST* new = (struct BST*) malloc(sizeof(struct BST)); new-\u0026gt;left = NULL; new-\u0026gt;right = NULL; return new; }; void Insert(struct BST** RootPtr, int value) { struct BST* temp = *RootPtr; if (temp == NULL) { /*When list is empty*/ struct BST* NewNode = CreateNode(); NewNode-\u0026gt;data = value; *RootPtr = NewNode; } else if (value \u0026lt;= temp-\u0026gt;data) { /*If user value is less then current node value insert in left of the node...*/ struct BST* NewNode = CreateNode(); NewNode-\u0026gt;data = value; temp-\u0026gt;left = NewNode; } else { /*If user value is greater then current node value insert at right of the node*/ struct BST* NewNode = CreateNode(); NewNode-\u0026gt;data = value; temp-\u0026gt;right = NewNode; } } int Search(struct BST* RootPtr, int item) { /*Implemented search using recursion*/ if(RootPtr == NULL) { return 0; /*Returns 0 if list is empty*/ } else if(item == RootPtr-\u0026gt;data) { return 1; /*Returns 1 when element found*/ } else if(item \u0026lt; RootPtr-\u0026gt;data) { Search(RootPtr-\u0026gt;left, item); /*Otherwise search in left side of binary tree if searching value is less then the current node value*/ } else { Search(RootPtr-\u0026gt;right, item); /*Otherwise search in right side of binary tree if searching value is greater then the current node value*/ } } void main() { struct BST* RootPtr = NULL; int item, cont, key; do { printf(\u0026#34;Enter item: \u0026#34;); scanf(\u0026#34;%d\u0026#34;,\u0026amp;item); Insert(\u0026amp;RootPtr, item); printf(\u0026#34;\\n1 to keep inserting/ 0 to Exit: \u0026#34;); scanf(\u0026#34;%d\u0026#34;,\u0026amp;cont); } while(cont == 1); printf(\u0026#34;\\nEnter element to search: \u0026#34;); scanf(\u0026#34;%d\u0026#34;,\u0026amp;key); if(Search(RootPtr, key) == 0) { printf(\u0026#34;\\nFound\\n\u0026#34;); } else { printf(\u0026#34;\\nNot Found\\n\u0026#34;); } } ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/algorithom/algorithom/","title":"alogrithom"},{"content":"0.1 取得程式碼 本書的原始碼可以在GitHub上找到，網址是 https://github.com/dev-cafe/cmake-cookbook 。 開源程式碼遵循MIT許可：只要原始版權和許可聲明包含在軟體/原始碼的任何副本中，可以以任何方式重複使用和重新混合程式碼。 授權的全文可以在 https://opensource.org/licenses/MIT 中看到。\n為了測試原始碼，需要使用Git取得程式碼：\n主要的GNU/Linux發行版都可以透過套件管理器安裝Git。 也可以從Git專案網站 https://gitscm.com 下載二進位發行版，進行安裝。 MacOS上，可使用自製或MacPorts安裝Git。 Windows上，可以從git專案網站( https://git-scm.com )下載git執行安裝檔。 可以透過github桌面用戶端存取這些範例，網址為 https://desktop.github.com 。\n另一個選擇是從 https://github.com/dev-cafe/cmake-cookbook 下載zip檔。\n安裝Git後，可以將遠端庫克隆到本機，如下所示：\n$ git clone https://github.com/dev-cafe/cmake-cookbook.git 這將建立一個名為cmake-cookbook的資料夾。 本書內容與原始碼的章節對應，書中章節的編號和源碼的順序相同。\n在GNU/Linux、MacOS和Windows上，使用最新的持續整合進行測試。 我們會在之後討論測試的設定。\n我們用標籤v1.0標記了與本書中列印的範例相對應的版本。 為了與書中內容對應，可以如下取得此特定版本：\n$ git clone --single-branch -b v1.0 https://github.com/dev-cafe/cmake-cookbook.git 我們希望收到Bug修復，並且Github庫將繼續發展。 若要取得更新，可以選擇庫的master分支。\n","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/cmake/cmake/","title":"Cmake"},{"content":"常見資料夾命名 utils 通常用來放置一些通用文件，可在多個專案重複使用，提升開發效率。\nsrc 指source code，裡面用來存放專案的程式碼。\nlib library,用來存放庫。\ndist distribution，用來放編譯過後的文件。\nbuild 放置構建後的文件。\nconfig 存放一些基本設定，像是編譯方式之類的。\nassets 存放一些專案會用到的圖片。\nexample,demo 用來放專案的使用範例，可以在一些庫的裡面發現。\nbin 命令腳本，若使用命令工具會遇到，有些需要加入windows環境變數，用來操作command line。\n","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/commen-sence/","title":"commen sence"},{"content":"學習如何使用git的各種命令和概念 git簡介 git是一種版本管理工具，在寫程式時可以像玩遊戲一樣設立存檔點，方便日後程式出問題時有機會回復到之前的某個狀態，也方便團隊做開發，對大一點 的程式尤為重要，其中又些更複雜的觀念，熟悉後才可提高日後的開發和協作效率。\ngit 下載 須先下載: $ https://git-scm.com/downloads\ngit基本命令(未考慮github時) git add. ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/git/","title":"git tutorial"},{"content":"pytorch 概念說明 先將訓練資料存入dataset→然後再將資料轉換成tensor格式\n資料處理 資料讀取原理\ndataset的製作(可以從這裡開始看，非常重要)\nTensorboard\ntransform\nCompose的用法(transform的class)\npytprch內建dataset的使用\ndataloader\n神經網路基本架構\n訓練模型 CNN\nRNN\nSVM\nDNN(deep learning)\nreference\npython的基礎知識\n皮膚癌預測紀錄\n","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/","title":"pytorch"},{"content":"CNN 捲積神經網路Convolutional Neural Network 捲積神經網路(Convolutional Neural Network, 簡稱CNN)是深度學習非常經典的一個模型，是一種主要用於圖像處理的學習模型。捲積神經網路的設計靈感來自於人類視覺系統的運作方式，能夠有效地處理和理解圖像資料。\n卷積神經網路利用線性運算（像是矩陣乘法）來獲得部分圖像的特徵。所以對計算有一定的需求。\n卷積層(Convolution Layers)\n池化層(pooling layers)\n非線性激活\n線性層及其他層\nCNN(python)\n","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/cnn-5d1d3da611ad4f5ea54e5dcdd4f71d12/","title":"pytorch"},{"content":"CNN(python) import numpy as np from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten from tensorflow.keras.layers import Conv2D, MaxPooling2D from tensorflow.keras.datasets import mnist from tensorflow.python.keras.utils import np_utils from matplotlib import pyplot as plt def load_data(): (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) x_test = x_test.reshape(x_test.shape[0], 28, 28, 1) x_train = x_train.astype(\u0026#39;float32\u0026#39;) x_test = x_test.astype(\u0026#39;float32\u0026#39;) y_train = np_utils.to_categorical(y_train, 10) y_test = np_utils.to_categorical(y_test, 10) return (x_train, y_train), (x_test, y_test) (x_train, y_train), (x_test, y_test) = load_data() model = Sequential() model.add(Conv2D(filters=3, kernel_size=(3,3), activation=\u0026#39;relu\u0026#39;, input_shape=(28,28,1), data_format=\u0026#34;channels_last\u0026#34;)) model.add(MaxPooling2D(pool_size=(2,2))) model.add(Flatten()) model.add(Dense(units=20, activation = \u0026#39;relu\u0026#39;)) model.add(Dense(units=20, activation = \u0026#39;relu\u0026#39;)) model.add(Dense(units=10, activation=\u0026#39;softmax\u0026#39;)) model.compile(loss=\u0026#39;categorical_crossentropy\u0026#39;, optimizer=\u0026#39;adagrad\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) #進行訓練, 訓練過程會存在 train_history 變數中 train_history = model.fit(x=x_train, y=y_train, epochs=5, batch_size=200) #顯示測試成果(testing data) result = model.evaluate(x_test, y_test) print(\u0026#34;\\nAccuracy of testing data = \u0026#34;, result) ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/cnn-5d1d3da611ad4f5ea54e5dcdd4f71d12/cnnpython-fab8b1dd0a7b4f46ac8ea5f6abce5269/","title":"pytorch"},{"content":"卷積層(Convolution Layers) 目的:卷積層是捲積神經網絡的核心，可提取圖片特徵值。 在捲積層經過掃描運算後會得到一張特徵圖(Feature Map)，特徵圖反映了在原始圖像中不同位置的特徵。 剩下兩層分別是池化層(Pooling Layer)和全連接層(Fully Connected Layer)。 卷積層(Convolution Layers)設定 nn.Conv2d:處理2維資料(較常用) Conv2d 參數: in_channels(int):輸入的通道數\nout_chennels(int):輸出的通道數，若是輸出兩個，則生成兩個卷積核，然後做疊加\nkernel_size(int or tuple):卷積核的大小，例如:設定2則出現2*2的卷積核\nstride(int or tuple):卷積過程中單次移動的路徑\npadding(int,tuples or str):是否需要針對圖片邊緣進行填充default:0\ndialation(int or tuple):default:1\ngroups(int):通常設1\nbias(bool):卷積做完後是否加上一個數字(模型自己會算)，通常True\npadding_mode(str):padding為True時決定以甚麼樣的模式填充，default為’zeros’(replacements:'zeros', 'reflect', 'replicate' or 'circular')\n原理: 動畫示範:https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\nmodules的使用方式: import torchvision from torch import nn from torch.utils.data import DataLoader dataset_transform = torchvision.transforms.Compose([ torchvision.transforms.ToTensor() ]) dataset = torchvision.datasets.CIFAR10(root=\u0026#34;./dataset\u0026#34;,train=False,transform=dataset_transform,download=False) data_loader = DataLoader(dataset=dataset,batch_size=64,shuffle=True,num_workers=0,drop_last=False) #初始化container class mymodules(nn.Module): def __init__(self): super(mymodules,self).__init__() self.conv1 = nn.Conv2d(in_channels=3,out_channels=6,kernel_size=3,stride=1,padding=0) def forward(self, x): x = self.conv1(x) return x #輸出測試: Mymodules = mymodules() print(Mymodules) #匯入圖片 for data in data_loader: imgs,labels = data output = Mymodules(imgs) print(output.shape) 輸出結果: 因為kernel size=33，每次移動一格，不會超過邊界，所以最後尺寸才會從3232變成30*30 匯入圖片的輸出結果長這樣: 第一項顯示batch size,第二項為chennels,後面是長跟高 將結果顯示到tensorboard 輸出設為channels=6，會無法顯示，需要轉換channels到3，才可被writer讀到 import torchvision import torch from torch import nn from torch.utils.data import DataLoader from torch.utils.tensorboard import SummaryWriter dataset_transform = torchvision.transforms.Compose([ torchvision.transforms.ToTensor() ]) dataset = torchvision.datasets.CIFAR10(root=\u0026#34;./dataset\u0026#34;,train=False,transform=dataset_transform,download=False) data_loader = DataLoader(dataset=dataset,batch_size=64,shuffle=True,num_workers=0,drop_last=False) writer = SummaryWriter(log_dir=\u0026#34;container\u0026#34;) #初始化container class mymodules(nn.Module): def __init__(self): super(mymodules,self).__init__() self.conv1 = nn.Conv2d(in_channels=3,out_channels=6,kernel_size=3,stride=1,padding=0) def forward(self, x): x = self.conv1(x) return x Mymodules = mymodules() print(Mymodules) #匯入圖片 step = 0 for data in data_loader: imgs,labels = data #print(imgs.shape) #print:torch.Size([64, 3, 32, 32]) #writer.add_images(\u0026#34;cnn_input\u0026#34;,imgs,step) output = Mymodules(imgs) #print(output.shape) #print:torch.Size([64, 6, 30, 30])，channel變成6，writer會寫不進去(需要chennel=3)，需要reshape做變換 output = torch.reshape(output,(-1,3,30,30)) #確認output_shape: print(output.shape) writer.add_images(\u0026#34;cnn_output\u0026#34;,output,step) step = step+1 writer.close() 因為channels有做改變，所以batch size也會不一樣，從6變到3，size會變大，若是不確定，可以輸入-1(第一項是batch size)，電腦會根據後面的值自動計算 輸出長這樣: batch size(64,6,30,30)→(128,3,30,30)，size變大2倍 圖片變化原理(和下面shape公式有關) 經過神經層(convolution)後通道數由3變成64 shape公式: N代表輸入的batch size，會根據公式重新計算新的高跟寬，所以最後算出3030，原本的dataset的圖片應該是3232才對 ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/cnn-5d1d3da611ad4f5ea54e5dcdd4f71d12/%E5%8D%B7%E7%A9%8D%E5%B1%A4convolution-layers-a66f75d937f44302a2aa191082ffe5c6/","title":"pytorch"},{"content":"池化層(pooling layers) 目的:池化層通常在卷積層之後，它的主要作用是減少特徵映射的空間尺寸，同時保留重要的特徵信息，降低計算成本。 常見的做法有最大池化(Max Pooling)或平均池化(Average Pooling)，最大池化是在區域內選最大的值作為代表，而平均池化則取區域內值的平均。 提升模型的運作效率，減少過擬和的風險，還可以增加模型的平移不變性(Translation Invariance，即使輸入圖像中的特徵稍微移動，池化層仍然能夠識別到相同的特徵。能夠提高模型對物體位置變化的容忍度)。 最大池化設定 torch.nn.MaxPool2d參數: kernel_size (Union[int, Tuple[int(高), int(寬)]]):給整數n，生成n*n大小視窗，或是給Tuple，自己決定高跟寬\nstride (Union[int, Tuple[int, int]]):移動距離，默認值為kernel size\npadding (Union[int, Tuple[int, int]]):Implicit negative infinity padding to be added on both sides\ndilation (Union[int, Tuple[int, int]]):a parameter that controls the stride of elements in the window\nreturn_indices (bool):if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later\nceil_mode (bool):取到圖片邊界時，kernel的尺寸會超過圖片，此時是否要取不完整的kernel，為true時會進行保留。\n補充:stride的移動方式: 補充:dialtion的移動方式: 決定每個視窗的元素間差多少間隔，可以輸入整數n，代表高、寬元素間都插入n個空格;或是用(m,n)指定高跟寬。圖片中是用1來插入。\n自訂義矩陣池化: #以上面二維矩陣舉例 import torch from torch import nn matrix = torch.tensor([ [2,3,0,5,2.5], [2,1.5,0.5,0,7], [1.5,5,5,3,2], [3,5,7,1.5,0], [2,5,2,1.5,2] ]) matrix = torch.reshape(matrix,(-1,1,5,5)) #-1會自動計算batch_size #後面代表層數，還有5*5 print(\u0026#34;matrix shape:\u0026#34;,matrix.shape,\u0026#39;\\n\u0026#39;) #試試神經網路 class mymodules(nn.Module): def __init__(self): super(mymodules,self).__init__() self.maxpool2 = nn.MaxPool2d(kernel_size=3,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=True) def forward(self, x): x = self.maxpool2(x) return x Mymodules = mymodules() matrix = Mymodules(matrix) 一般來說圖片中都有小數，所以默認是用浮點數運算，若是輸入的矩陣為整數，可以在矩陣後面加上dtype(float32) 圖片池化 import torch from torch import nn import torchvision from torch.utils.data import DataLoader from torch.utils.tensorboard import SummaryWriter dataset_transform = torchvision.transforms.Compose([ torchvision.transforms.ToTensor() ]) dataset = torchvision.datasets.CIFAR10(root=\u0026#34;./dataset\u0026#34;,train=False,transform=dataset_transform,download=False) data_loader = DataLoader(dataset=dataset,batch_size=64,shuffle=True,num_workers=0,drop_last=False) #試試神經網路 class mymodules(nn.Module): def __init__(self): super(mymodules,self).__init__() self.maxpool2 = nn.MaxPool2d(kernel_size=3,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=True) def forward(self, x): x = self.maxpool2(x) return x Mymodules = mymodules() #讀到tensorboard來看看 writer = SummaryWriter(\u0026#34;maxpool\u0026#34;) step = 0 for data in data_loader: imgs,lables = data output = Mymodules(imgs) writer.add_images(\u0026#34;original_data\u0026#34;,imgs,step) writer.add_images(\u0026#34;maxpool_output\u0026#34;,output,step) step = step+1 writer.close() 輸出結果: 圖片容量變小(變模糊)，可以加快模型速度 ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/cnn-5d1d3da611ad4f5ea54e5dcdd4f71d12/%E6%B1%A0%E5%8C%96%E5%B1%A4pooling-layers-64f5836035104538aca6ecfcf11d84eb/","title":"pytorch"},{"content":"線性層及其他層 ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/cnn-5d1d3da611ad4f5ea54e5dcdd4f71d12/%E7%B7%9A%E6%80%A7%E5%B1%A4%E5%8F%8A%E5%85%B6%E4%BB%96%E5%B1%A4-20c8a2790edf41b7851c8ad32d1b43c5/","title":"pytorch"},{"content":"非線性激活 目的 為神經元導入非線性算法，使其可以逼近任何非線性函數，如果不使用，模型便會使用線性方式做預測，使用就會受限許多，常見的非線性模式有RELU，Sigmoid兩種。\nRELU使用 會畫出像上面這種圖形 inplace的解說: import torch from torch import nn from torch.nn import ReLU matrix = torch.tensor([ [1,-0.5], [-1,3] ]) #改成一維的2*2矩陣，-1是自動計算batch size matrix = torch.reshape(matrix,(-1,1,2,2)) #建立神經網路 class mymodules(nn.Module): def __init__(self): super(mymodules,self).__init__() self.relu1 = ReLU() def forward(self, x): x = self.relu1(x) return x Mymodules = mymodules() output = Mymodules(matrix) print(output) 輸出結果: Sigmoid 使用效果 import torch from torch import nn from torch.nn import ReLU,Sigmoid from torch.utils.tensorboard import SummaryWriter from torch.utils.data import DataLoader import torchvision dataset_transform = torchvision.transforms.Compose([ torchvision.transforms.ToTensor() ]) test_set = torchvision.datasets.CIFAR10(root=\u0026#34;./dataset\u0026#34;,train=False,transform=dataset_transform,download=False) test_loader = DataLoader(dataset=test_set,batch_size=64,shuffle=True,num_workers=0,drop_last=False) writer = SummaryWriter(\u0026#34;sigmoid\u0026#34;) #建立神經網路 class mymodules(nn.Module): def __init__(self): super(mymodules,self).__init__() self.relu1 = ReLU() self.sigmoid1 = Sigmoid() def forward(self, x): x = self.sigmoid1(x) return x Mymodules = mymodules() step = 0 for data in test_loader: imgs,labels = data writer.add_images(\u0026#34;origin\u0026#34;,imgs,step) output = Mymodules(imgs) writer.add_images(\u0026#34;sigmoid\u0026#34;,output,step) step = step+1 writer.close() 顯示結果: ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/cnn-5d1d3da611ad4f5ea54e5dcdd4f71d12/%E9%9D%9E%E7%B7%9A%E6%80%A7%E6%BF%80%E6%B4%BB-11b4c2014a504f1795a8dc7e736d9daf/","title":"pytorch"},{"content":"Compose的用法(transform的class) 屬於transforms裡的一種class，可以將資料處理方式包裝在裡面，以列表方式依序執行，就不用寫這麼多步驟了(Args:list of transform object)，因為list所以要中括號 可將resize,totensor,normalize放在裡面，所述入的引數均為class object的數據類型儘量保持一致，不要一下PIL一下又變成tensor 這裡一樣是說明如何將圖片轉換成tensor並且修改尺寸 #之前一樣要宣告totensor,resize trans_totensor = transforms.ToTensor() trans_norm = transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5]) trans_resize = transforms.Resize((512,512)) #對compose的class做初始化設定 trans_compose = transforms.Compose([trans_totensor,trans_norm,trans_resize]) ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/compose%E7%9A%84%E7%94%A8%E6%B3%95transform%E7%9A%84class-2565a397437e445d93e158802b38b8ad/","title":"pytorch"},{"content":"dataloader 將dataset載入到裡面，方便模型使用 是一種class，需載入多個參數，可以迭代讀取: dataset:載入的dataset名稱 batch_size(int,optional)：每次載入數據的大小(每一個batch) shuffle(bool,optional)：是否打亂數據(default:False)，一般設True，先打亂在讀取 sampler batch_sampler num_workers(int,optional)：數據載入的並行度(線程使用)，越多載入越快，默認為0，但windows下超過0會報錯，0代表主線程 drop_last(bool,optional):當dataset的資料個數和batch單次取的數量無法整除時，剩下的是否放棄 #使用torchvision的dataset from torch.utils.data import DataLoader from torch.utils.tensorboard import SummaryWriter import torchvision dataset_transform = torchvision.transforms.Compose([ torchvision.transforms.ToTensor() ]) test_set = torchvision.datasets.CIFAR10(root=\u0026#34;./dataset\u0026#34;,train=False,transform=dataset_transform,download=False) test_loader = DataLoader(dataset=test_set,batch_size=4,shuffle=True,num_workers=0,drop_last=False) img,label = test_set[0] print(img.shape) classes = test_set.classes[label] print(classes) for data in test_loader: imgs,labels = data print(imgs.shape) print(test_set.classes[labels]) 前面從test_set取了第一張圖片和其label，並且將其印出來 然後使用遍歷的方式將test_loader的資料一筆一筆讀出來 將dataloader讀進tensorboard from torch.utils.data import DataLoader from torch.utils.tensorboard import SummaryWriter import torchvision dataset_transform = torchvision.transforms.Compose([ torchvision.transforms.ToTensor() ]) test_set = torchvision.datasets.CIFAR10(root=\u0026#34;./dataset\u0026#34;,train=False,transform=dataset_transform,download=False) test_loader = DataLoader(dataset=test_set,batch_size=4,shuffle=True,num_workers=0,drop_last=False) writer = SummaryWriter(\u0026#34;dataloader\u0026#34;) step = 0 for data in test_loader: imgs,labels = data writer.add_images(\u0026#34;dataloader\u0026#34;,imgs,step) step = step+1 writer.close() 注意:要用add_images，非imge(因為batch為4，所以shape變成(4,3,32,32)(batchsize,channel,width,length)，原本沒有batchsize這一格) 輸出結果: 將dataloader取兩輪(epoch)，每次64張 from torch.utils.data import DataLoader from torch.utils.tensorboard import SummaryWriter import torchvision dataset_transform = torchvision.transforms.Compose([ torchvision.transforms.ToTensor() ]) test_set = torchvision.datasets.CIFAR10(root=\u0026#34;./dataset\u0026#34;,train=False,transform=dataset_transform,download=False) test_loader = DataLoader(dataset=test_set,batch_size=64,shuffle=True,num_workers=0,drop_last=False) writer = SummaryWriter(\u0026#34;dataloader\u0026#34;) step = 0 for epoch in range(2) for data in test_loader: imgs,labels = data writer.add_images(\u0026#34;Epoch:{}\u0026#34;.format(epoch),imgs,step) step = step+1 writer.close() step很高，因為是一次取64張，然後跑完dataloader，然後再做一次 輸出結果: ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/dataloader-3000ab06a65c4b238607c4ee4f9e67bf/","title":"pytorch"},{"content":"dataset的製作(可以從這裡開始看，非常重要) 製作一個class，將dataset存在裡面 用之前的方法將文件夾路徑、和包含圖片名稱的list合成，並且將圖片完整路徑使用opencv或是PIL將其打開。 打開的圖片再轉換成tensor格式，然後就留在class裡面。 from torch.utils.data import Dataset, DataLoader from PIL import Image import os from torchvision import transforms class MyData(Dataset): def __init__(self, root_dir, image_dir, label_dir, transform=None): self.root_dir = root_dir self.image_dir = image_dir self.label_dir = label_dir self.label_path = os.path.join(self.root_dir, self.label_dir) self.image_path = os.path.join(self.root_dir, self.image_dir) self.image_list = os.listdir(self.image_path) self.label_list = os.listdir(self.label_path) self.transform = transform # 因为label 和 Image文件名相同，进行一样的排序，可以保证取出的数据和label是一一对应的 self.image_list.sort() self.label_list.sort() def __getitem__(self, idx): img_name = self.image_list[idx] label_name = self.label_list[idx] img_item_path = os.path.join(self.root_dir, self.image_dir, img_name) label_item_path = os.path.join(self.root_dir, self.label_dir, label_name) img = Image.open(img_item_path) with open(label_item_path, \u0026#39;r\u0026#39;) as f: label = f.readline() if self.transform: img = transform(img) return img, label def __len__(self): assert len(self.image_list) == len(self.label_list) return len(self.image_list) #讀取圖片: img,label = Mydata[idx] 裡面的init,getitem,len原本是Dataset的函數，這裡做重新定義的工作(說明文件要求的) 當圖片都存入到class後，可以把兩個不同的dataset直接相加，像是:new_dataset=dataset1+dataset2(因為Dataset裡面__add__()的定義?) 呼叫的時候直接用dataset = Mydataset[index]就可以了(因為有getitem)，需要讀出len才可遍歷(此時不輸入參數，默認使用len)→overwritting ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/dataset%E7%9A%84%E8%A3%BD%E4%BD%9C%E5%8F%AF%E4%BB%A5%E5%BE%9E%E9%80%99%E8%A3%A1%E9%96%8B%E5%A7%8B%E7%9C%8B%E9%9D%9E%E5%B8%B8%E9%87%8D%E8%A6%81-302063e805d74a48bbdf113ba8cb17fe/","title":"pytorch"},{"content":"DNN(deep learning) DNN(python)\n","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/dnndeep-learning-809b1debc15f4d09a236679e079deb19/","title":"pytorch"},{"content":"DNN(python) import numpy as np import pandas as pd from matplotlib import pyplot as plt from tensorflow.keras.models import Sequential from tensorflow.keras.datasets import mnist from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten from tensorflow.python.keras.utils import np_utils df=pd.read_csv(\u0026#34;lec01_pm25data.csv\u0026#34;) data=df[[\u0026#39;風速(m/sec)\u0026#39;,\u0026#39;細懸浮微粒(μg/m3)\u0026#39;]] data=data.iloc[0:18000] data=data.dropna() x_train = data[[\u0026#39;風速(m/sec)\u0026#39;]].iloc[0:15000] y_train = data[\u0026#39;細懸浮微粒(μg/m3)\u0026#39;].iloc[0:15000] x_test = data[[\u0026#39;風速(m/sec)\u0026#39;]].iloc[15000:18000] y_test = data[\u0026#39;細懸浮微粒(μg/m3)\u0026#39;].iloc[15000:18000] x_train = x_train.to_numpy() y_train = y_train.to_numpy() x_test = x_test.to_numpy() y_test = y_test.to_numpy() model = Sequential() #建立神經網路第一層，1個神經元，輸入為1個數值 model.add(Dense(units=1, input_shape=(1,), activation = \u0026#39;linear\u0026#39;)) #訓練神經網路，使用何種loss function, 何種gradient descent model.compile(loss=\u0026#39;mse\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;mape\u0026#39;]) #進行訓練, 訓練過程會存在train_history train_history = model.fit(x=x_train, y=y_train, epochs=20, batch_size=100) #顯示測試效果，用來知道準確度(testing data) result = model.evaluate(x_test, y_test) print(\u0026#34;\\nAccuracy of testing data = \u0026#34;, result) #預測 predictions = model.predict(x_test) print(\u0026#39;\\n\u0026#39;, predictions) ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/dnndeep-learning-809b1debc15f4d09a236679e079deb19/dnnpython-55cc8a1c4d444ac9927be40fdab694ca/","title":"pytorch"},{"content":"python的基礎知識 基礎的資料儲存方式 list 是一種class，裡面定義了很多方法\nlist.append(x):將x加到list後面\nlist.insert(i,x):第一個為索引值，將x加到第i個digits\na.inset(len(a),x)代表的和list.append(x)一樣意思 list.remove(x):刪除list中第一個為x的元素，若是沒有會出現ValueError\nlist.clear():刪除list中所有元素\nlist.count(x):x在list中出現次數\nlist.reverse():將所有元素前後反轉\nlist.sort():list進行排序\nlist.replace(”L”,”l”):將字串中的L替換成l\n字串完成後前方加上(int)可以將字串轉換成整數(也可以用float)\nlist1 = input(\u0026#34;please enter a number\u0026#34;) list2 = input(\u0026#34;please enter a number\u0026#34;) print(list1+list2) #若輸入3,5，出來會變成35 #要改成這樣: print((int)list1+(int)list2) 若是字串輸出: list1 = [\u0026#34;hello\u0026#34;,\u0026#34;HI\u0026#34;,\u0026#34;你好\u0026#34;] print(list1[0]) #輸出hello print(list[0:]) #從0輸出到最後一個數 關於[:]的用法(切片操作): list = {\u0026#34;h\u0026#34;,\u0026#34;e\u0026#34;,\u0026#34;l\u0026#34;,\u0026#34;l\u0026#34;,\u0026#34;o\u0026#34;} #list[i:j]指的是讀取list下標i到j的元素，且從0開始算 #若是i省略會被0取代；若是j省略會被len(list)取代，也就是會變成list長度 #[:j]此時也代表0到j之前的元素才會被讀取，不包含j，若是j為3會輸出hel(非常莫名其妙，要特別理解) #若是使用[3:]代表讀取第四個元素到最後一個元素 Tuples 和list的用法很像，只是存放時是用小括號 Dictionary 跟list使用數字當索引不同，他的索引可以是數字或是字串等等，比較像是set的概念 物件導向 基礎寫法: class MyData(Dataset): def __init__(self, root_dir, image_dir, label_dir, transform=None): self.root_dir = root_dir self.image_dir = image_dir self.label_dir = label_dir self.label_path = os.path.join(self.root_dir, self.label_dir) self.image_path = os.path.join(self.root_dir, self.image_dir) self.image_list = os.listdir(self.image_path) self.label_list = os.listdir(self.label_path) self.transform = transform # 因为label 和 Image文件名相同，进行一样的排序，可以保证取出的数据和label是一一对应的 self.image_list.sort() self.label_list.sort() def __getitem__(self, idx): img_name = self.image_list[idx] label_name = self.label_list[idx] img_item_path = os.path.join(self.root_dir, self.image_dir, img_name) label_item_path = os.path.join(self.root_dir, self.label_dir, label_name) img = Image.open(img_item_path) with open(label_item_path, \u0026#39;r\u0026#39;) as f: label = f.readline() if self.transform: img = transform(img) return img, label def __len__(self): assert len(self.image_list) == len(self.label_list) return len(self.image_list) magic method(special method):像是__init__,getitem,__len__就是這一種，python裡面有建立，有特殊的使用方法，像是init的功能是初始化變數(可以理解成建構子)，定義物件中(object)可有哪些變數可被存取，建立class一定要有這個函數。 the object is the instance of the class:還沒宣告之前都是class，宣告以後就變成object，有加上self(C中為this)才會知道那些變數可以被存取，這些算是object的變數，若是沒有加上self就不能被存取，這些算是class的變數，而且python比較特別，class裡面包裝的不只有function還有變數，可以同時兼顧功能跟資料型態的包裝。不像是C有分成struct跟class，將方法跟儲存分開。 class後面的括號通常默認是object，若是用別的class名字代替，代表繼承，此時這個class可以使用括號裡面的class的函數跟變數，此時自己稱作子類別，給予的稱作父類別。若是變數不想被繼承，可以在函數或變數之前寫”__variable”，代表private。 ref:https://docs.python.org/zh-tw/3/tutorial/datastructures.html#dictionaries\n","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/python%E7%9A%84%E5%9F%BA%E7%A4%8E%E7%9F%A5%E8%AD%98-b10a35da7cf14ebdb021c48de47cf7a3/","title":"pytorch"},{"content":"pytprch內建dataset的使用 使用CIFAR10資料集 import torchvision train_set = torchvision.datasets.CIFAR10(root=\u0026#34;./dataset\u0026#34;,train=True,download=True) test_set = torchvision.datasets.CIFAR10(root=\u0026#34;./dataset\u0026#34;,train=False,download=True) #查看內容 print(test_set[0]) print(test_set.classes) img, label = test_set[0] print(img) print(label) print(test_set.classes[label]) img.show() root用來存放訓練集所在資料夾 download用來決定是否下載資料集(看電腦原本有沒有，沒有會自動從官網下載) train:用來決定是訓練集或測試集 CIFAR10裡面包含10種分類集 輸出結果: test_set[0]裡面是PIL檔案，0指的是測試集的第一個，後面那個3指的是package\ntest_set.classes代表裡面有多少種分類(classes是位於test_set這個object的一個list)\ntest_set[0]可返回兩種值img,label(因為是一種class)\n對dataset使用compose from torch.utils.tensorboard import SummaryWriter import torchvision dataset_transform = torchvision.transforms.Compose([ torchvision.transforms.ToTensor() ]) train_set = torchvision.datasets.CIFAR10(root=\u0026#34;./dataset\u0026#34;,train=True,transform=dataset_transform,download=True) test_set = torchvision.datasets.CIFAR10(root=\u0026#34;./dataset\u0026#34;,train=False,transform=dataset_transform,download=True) writer = SummaryWriter(\u0026#34;logs\u0026#34;) for i in range(10): img,label = test_set[i] writer.add_image(\u0026#34;p10\u0026#34;,img,i) writer.close() ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/pytprch%E5%85%A7%E5%BB%BAdataset%E7%9A%84%E4%BD%BF%E7%94%A8-a6ba7d74c2b942508f822e3d387de155/","title":"pytorch"},{"content":"reference https://ithelp.ithome.com.tw/m/articles/10328869 https://www.youtube.com/watch?v=OP5HcXJg2Aw\u0026amp;list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J\u0026amp;index=9 https://www.youtube.com/watch?v=1_bwX0shTd8 https://pytorch.org/docs/stable/nn.html#pooling-layers ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/reference-96c7b6e2ef3b438c8f6b3f9824f48f7f/","title":"pytorch"},{"content":"RNN KNN(python)\n","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/rnn-b352723236c34dbdadcc9735bf093d07/","title":"pytorch"},{"content":"KNN(python) from sklearn import datasets from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn import metrics from sklearn.neighbors import KNeighborsClassifier digits = datasets.load_digits() #切割訓練資料與測試資料 x_train, x_test = digits.data[0:1000,:], digits.data[1200:1700,:] y_train, y_test = digits.target[0:1000], digits.target[1200:1700] #my program about dealing classification knn = KNeighborsClassifier(n_neighbors=5) knn.fit(x_train, y_train) ### y_pred = knn.predict(x_test) #或是y_pred = svm.predict(x_test) print(y_pred,\u0026#39;\\n\u0026#39;) # print the confusion matrix print(metrics.confusion_matrix(y_test, y_pred)) ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/rnn-b352723236c34dbdadcc9735bf093d07/knnpython-e43b05cba91441e786026cd98f2606fe/","title":"pytorch"},{"content":"SVM SVM(python)\n","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/svm-840bfe110f9646308b665cf148333537/","title":"pytorch"},{"content":"SVM(python) from sklearn import datasets from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn import metrics from sklearn.neighbors import KNeighborsClassifier digits = datasets.load_digits() #切割訓練資料與測試資料 x_train, x_test = digits.data[0:1000,:], digits.data[1200:1700,:] y_train, y_test = digits.target[0:1000], digits.target[1200:1700] #my program about dealing classification #SVM相關設定，StandardScaler會對資料x進行標準化，標準化可改善辨識結果 svm = make_pipeline(StandardScaler(), SVC(gamma=\u0026#39;auto\u0026#39;)) #進行SVM運算 svm.fit(x_train,y_train) ### y_pred = svm.predict(x_test) #或是y_pred = svm.predict(x_test) print(y_pred,\u0026#39;\\n\u0026#39;) # print the confusion matrix print(metrics.confusion_matrix(y_test, y_pred)) ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/svm-840bfe110f9646308b665cf148333537/svmpython-d443a347918e49dbb372b93fb142cca8/","title":"pytorch"},{"content":"Tensorboard 簡介: 可以將loss變化過程，input,output圖像化:\n使用方法: from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter(\u0026#34;logs\u0026#34;) #將對應的事件文件存在logs文件夾底下，沒有會自己新增 writer.add_image() #新增圖片 writer.add_scalar() #新增標量 writer.close() ps.SummaryWriter是一個class，這裡是在宣告建立一個class\n開啟方法: 要執行tensorboard需要有訓練的紀錄檔，將項目資料夾指向logs資料夾(之前建的)，logdir(系統參數)是紀錄文件的文件夾所在位置(通常放在logs資料夾)\ntensorboard --logdir=logs 執行的時候是在port上輸出，若是有衝突的話可以修改port tensorboard --logdir=logs --port=6007 scalar範例: the arguments of add_scalar(self,tag,scalar_value,global_step=None,walltime=None): tag:輸出結果標題，若是名稱相同會自動分到同一類，此時global_step會決定順序\nscalar_value:y軸\nglobal_step:x軸\n輸出一個y=x的圖形在tensorboard上 from torch.utils.tensorboard import SummaryWriter for i in range(100): writer.add_scalar(\u0026#34;y=x\u0026#34;,i,i) writer.close() ps.圖形可能會出現互相覆蓋的情況，可以把之前的紀錄檔刪掉，或是在建立一個新的資料夾。\n圖片範例: 函數定義: def add_image(self, tag, img_tensor, global_step=None, walltime=None, dataformats=\u0026#34;CHW\u0026#34;) img_tensor:輸入的圖像，有規定的類型(torch.tensor,numpy.array)，且有規定尺寸(dataformats改成) global_step:x軸(對於顯示圖片來說會需要再tensorboard選擇要觀看的step，因為圖片和數字不同，無法直接表達連續性)，然後使用者可以觀察不同step圖片是如何變化的。 shap解決: 若是使用PIL到numpy，需要再add_image()中指定shap中每一個數字/維度所代表意義。\n解決方法:\n可以使用print(img_array.shap)查看格式，看通道放的位置，若是(512,768,3)，代表(H,W,C)(高度，寬度，通道)dataformats改成’HWC’(默認是CHW)\nfrom torch.utils.tensorboard import SummaryWriter from PIL import Image import numpy as np writer = SummaryWriter(\u0026#34;logs\u0026#34;) #將對應的事件文件存在logs文件夾底下，沒有會自己新增 img_path = \u0026lt;path\u0026gt; img = Image.open(img_path) #print(type(img)) #會回復PIL.JpegImageFile類型，需要轉換 img_array = np.array(img) writer.add_image(\u0026#34;show_picture\u0026#34;,img_array,1,dataformats=\u0026#39;HWC\u0026#39;) #新增圖片 writer.close() ps.若是使用PIL需要轉換檔案類型，使用opencv開啟圖片預設是numpy，會比較方便，具體如下:\nfrom torch.utils.tensorboard import SummaryWriter import cv2 as cv import numpy as np writer = SummaryWriter(\u0026#34;logs\u0026#34;) #將對應的事件文件存在logs文件夾底下，沒有會自己新增 img_path = \u0026#34;dataset/72908383_p0.png\u0026#34; img = cv.imread(img_path) print(type(img)) #會回復PIL.JpegImageFile類型，需要轉換 img_array = np.array(img) print(img_array.shape) writer.add_image(\u0026#34;show_picture\u0026#34;,img_array,1,dataformats=\u0026#39;HWC\u0026#39;) #新增圖片 writer.close() ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/tensorboard-1fb0c382b6ba4004b8ce591d6aa0932b/","title":"pytorch"},{"content":"transform 意義: 可以用來將PIL或是numpy.array的圖片裁減，並且轉換成可供模型使用的格式(tensor)，轉換方法如下:\nfrom PIL import Image from torchvision import transforms import numpy as np img_path = \u0026lt;path\u0026gt; img = Image.open(img_path) #print(type(img)) #會回復PIL.JpegImageFile類型，需要轉換 #img_array = np.array(img) tensor_trans = transforms.ToTensor() #宣告object tensor_img = tensor_trans(img) #使用object中的方法 ps.換成tensor可以直接使用PIL或是numpy.array。\nToTensor是一個class，因為此class中只有一個函數需要引數，所以會將引數帶入該函數，不用額外呼叫函數(此class需使用_call_函數來實現，可減少函數調用的麻煩)。也可以這樣用: img_path = \u0026#34;dataset/72908383_p0.png\u0026#34; img = Image.open(img_path) #print(type(img)) #會回復PIL.JpegImageFile類型，需要轉換 #img_array = np.array(img) transforms.ToTensor() tensor_img = transforms.ToTensor()(img) print(tensor_img) tensor裡面長真醜…\n正規化: 當圖片轉換成tensor後才使用 不同維度的特徵值可能不一樣，作正規化是希望值能夠接近，使梯度下降，更容易求解，加快模型運算速度 # #轉換成tensor的過程，變成tensor_img # #宣告trans_norm的object trans_norm = transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5]) #將tensor的img作正規化，變成img_norm img_norm = trans_norm(tensor_img) #CHW的關係，輸出第一層，第一行，第一列，這裡展示正規化的效果 print(tensor_img[0][0][0])#以上面來說會輸出0.0588 print(img_norm[0][0][0]) writer.add_image(\u0026#34;tensor\u0026#34;,tensor_img) writer.add_image(\u0026#34;normalize\u0026#34;,img_norm) normalize裡面放的是[m1,m2,…mn],[s1,s2,…,sn]，m指的是你希望的平均，s指的是你希望的標準差，n的個數取決於圖片通道數(Channel)，若是RGB那n=3，s,m數字要多少看經驗。\nps.tensor的儲存順序為CHW(通道,長,寬) 具體公式如下:input[channel] = (input[channel]-mean[channel])/std[channel]\n效果如下:\n尺寸修改(resize) 若給的尺寸是序列(包含寬度，高度)，圖片會調整成這個尺寸 若給的尺寸為一整數，圖片最小邊會等比縮放成該尺寸 可以輸入PIL或是tensor格式 # #tensor正規化的過程，得到img_norm # #宣告object，將引數值帶入到class的init方法中 trans_resize = transforms.Resize((512,512)) img_resize = trans_resize(img_norm) resize裡面裝的是整數或是tuples的格式，所以要記得加括號 效果像這樣: ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/transform-7fac7412a4944a2aae5bf9b8c4974304/","title":"pytorch"},{"content":"皮膚癌預測紀錄 程式碼: from torch.utils.tensorboard import SummaryWriter import torch from utils import Mydataset from torchvision import transforms import numpy as np from torch.utils.data import DataLoader import torchvision from utils import Mydataset #宣告重要變數 root_dir = \u0026#34;D:\\\\python program\\\\pytorch\\\\cancer_prediction\\\\train\u0026#34; label_Benign = \u0026#34;Benign\u0026#34; label_Malignant = \u0026#34;Malignant\u0026#34; writer = SummaryWriter(\u0026#34;logs\u0026#34;) #讀取dataset，並修改成tensor dataset_norm = transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5]) dataset_resize = transforms.Resize((512,512)) dataset_totensor = transforms.ToTensor() dataset_transform_compose = transforms.Compose([dataset_totensor,dataset_norm,dataset_resize]) Benign_dataset = Mydataset(root_dir,label_Benign,dataset_transform_compose) Malignant_dataset = Mydataset(root_dir,label_Malignant,dataset_transform_compose) #讀到loader裡面 Benign_loader = DataLoader(dataset=Benign_dataset,batch_size=4,shuffle=True,num_workers=0,drop_last=False) Malignant_loader = DataLoader(dataset=Malignant_dataset,batch_size=4,shuffle=True,num_workers=0,drop_last=False) ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/%E7%9A%AE%E8%86%9A%E7%99%8C%E9%A0%90%E6%B8%AC%E7%B4%80%E9%8C%84-23c5062e241b4c0492152cd2766c2a76/","title":"pytorch"},{"content":"神經網路基本架構 以torch.nn內的工具為主 ref:https://pytorch.org/docs/stable/nn.html\ncontainer中module的使用 提供神經網路模型最基礎的架構，所有模型都要用到 透過繼承來使用，然後重新定義init,forward函數 下面的Model是自己定義的，透過繼承nn.Module的方式 過程大概是這樣的:input→forward→output import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) forward中的x指的是輸入值，會經過一次卷積(conv1(x))，在經過一次非線性處理(relu)，然後再覆蓋x原來的值，回傳時又再做一次，差別在於兩次卷積的參數不同。 舉個栗子: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super().__init__() def forward(self, x): x+ = 1 return x network = Model() x = torch.tensor(1.0) output = network(x) print(output) 輸出結果: ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%A7%8B-b3be7cae5e9240a48fd5b0f4c3e586f5/","title":"pytorch"},{"content":"資料讀取原理 需要先將圖片路徑合成出來，在依照路徑個別讀取 圖片讀取(單張圖片、多張圖片) from PIL import Image import os #show the single picture image_path = \u0026#34;\u0026lt;image_path\u0026gt;\u0026#34; image = Image.open(image_path) image.show() #show the picture #read the multiple pictures from the directory img_path_list = os.listdir(\u0026lt;dir_path\u0026gt;) #用list存放圖片的名稱 img_path_list[0].show() PS.可以用這個方法將資料夾裡面的每一個檔案的路徑合成出來(路徑合成):\nimport os root_path = \u0026#34;dataset/train\u0026#34; label_path = \u0026#34;ants\u0026#34; path = os.path.join(root_path,label_path) #path = dataset/train/ants 具體多個檔案同時合成路徑的方法:\n先將資料夾所在路徑合成出來，再使用os的listdir工具(可以將資料夾內的所有檔案名存在list) 再將資料夾路徑，和存成list的檔案名們一起合成(此時是一對多的合成) import os def(self,root_dir,label_dir): #有了self才可使用現在指針中class裡的變量或是函式，確保不會用錯新的位置但相同定義的class self.root_dir = root_dir self.label_dir = label_dir self.path = os.path.join(self.root_dir,self.label_dir) #path = dataset/train/ants self.img_name = os.listdir(self.path) #將當前資料夾的所有檔案名顯示出來 img_path = os.path.join(self.path,self.img_name) #將原本的圖片名稱加上路徑合成出新的list ","date":"2024-01-27T02:12:31+08:00","permalink":"https://leo00932302.github.io/post/pytorch/pytorch-ddafb216569d48da8340c46a27254af7/%E8%B3%87%E6%96%99%E8%AE%80%E5%8F%96%E5%8E%9F%E7%90%86-f67a95d3f5e84ba0b38112e4c58f1fc3/","title":"pytorch"},{"content":"資料結構 想學好資料結構的第一步就要先知道，資料結構本身和演算法的關係密不可分，所以兩本課本內容相似的地方非常多，若要說差在哪裡， 資料結構重視資料的儲存方式，演算法在乎取用這些資料的方式。一個好的資料結構並不一定代表儲存空間越小，因為使用越小的儲存 空間可能會帶來取用的不便利，也就是說會加重演算法的負擔。而根據目的的不同選擇不同的演算法及資料結構，使其可應對大多數狀況 則是工程師的工作。\n簡單的排序問題: //the programe using c++ and vector #incude\u0026lt;iostream\u0026gt; using namespace std; void bubble_sort(vector\u0026lt;int\u0026gt;vec){ for(int ix=0;ix\u0026lt;vec.size();ix++){ for(int jx=ix;jx\u0026lt;vec.size();jx++){ if(vec[jx]\u0026gt;vec[jx+1]){ swap(vec[jx],vec[jx+1]); } } } } ","date":"2024-01-23T23:23:04+08:00","permalink":"https://leo00932302.github.io/post/data-structure/","title":"資料結構簡介"}]